{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# MapReduce Produce (MRP) - Agentic Workflow Demo\n",
        "\n",
        "This notebook demonstrates the complete **\"English → YAML DSL → Parallel Runtime\"** workflow of the MapReduce Produce system.\n",
        "\n",
        "## 🏗️ Architecture Overview\n",
        "\n",
        "```\n",
        "Natural Language Query → compile.py → YAML DSL → run_job.py → Parallel Execution\n",
        "```\n",
        "\n",
        "- **Translation Layer**: Maps natural language to structured specifications\n",
        "- **Runtime Engine**: Executes parallel map-reduce-produce workflows\n",
        "- **Agent Registry**: Pluggable processing functions\n",
        "- **Multiple Output Formats**: Console, JSON, Markdown reports\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 📦 Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Found MRP source at: /Users/hamidbagheri/GitHub/a10i/bcb_demo/src/mrp\n",
            "\n",
            "📁 Working with path: /Users/hamidbagheri/GitHub/a10i/bcb_demo/src/mrp\n",
            "📁 Current directory: /Users/hamidbagheri/GitHub/a10i/bcb_demo/notebooks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import yaml\n",
        "from IPython.display import display, Markdown, JSON\n",
        "\n",
        "# Try to find MRP source directory in different possible locations\n",
        "possible_mrp_paths = [\n",
        "    Path('../src/mrp').resolve(),  # Local development (from notebooks/)\n",
        "    Path('./src/mrp').resolve(),   # If running from root\n",
        "    Path('../../src/mrp').resolve(),  # If in nested folder\n",
        "    Path('/Users/hamidbagheri/GitHub/a10i/bcb_demo/src/mrp'),  # Absolute local path\n",
        "    Path.cwd() / 'src' / 'mrp',    # Current working directory\n",
        "    Path.cwd() / 'mrp',            # MRP files in current directory\n",
        "]\n",
        "\n",
        "mrp_path = None\n",
        "for path in possible_mrp_paths:\n",
        "    if path.exists() and (path / 'compile.py').exists():\n",
        "        mrp_path = path\n",
        "        print(f\"✅ Found MRP source at: {mrp_path}\")\n",
        "        break\n",
        "\n",
        "if mrp_path is None:\n",
        "    print(\"❌ Error: Could not find MRP source directory!\")\n",
        "    print(\"📁 Searched paths:\")\n",
        "    for path in possible_mrp_paths:\n",
        "        exists = \"✅\" if path.exists() else \"❌\"\n",
        "        print(f\"   {exists} {path}\")\n",
        "    \n",
        "    print(\"\\n💡 Solutions:\")\n",
        "    print(\"   1. Run this notebook from the notebooks/ directory\")\n",
        "    print(\"   2. Copy compile.py and run_job.py to your current directory\")\n",
        "    print(\"   3. Use the fallback demo below\")\n",
        "    \n",
        "    # Create a fallback demo with inline code\n",
        "    print(\"\\n🔧 Creating fallback demo setup...\")\n",
        "    mrp_path = Path.cwd()\n",
        "else:\n",
        "    # Add the MRP source directory to Python path\n",
        "    sys.path.insert(0, str(mrp_path))\n",
        "\n",
        "print(f\"\\n📁 Working with path: {mrp_path}\")\n",
        "print(f\"📁 Current directory: {Path.cwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ MRP modules imported successfully\n",
            "🔧 Available agents: ['process_bio']\n",
            "📊 Available reducers: ['stats']\n",
            "📤 Available producers: ['print', 'save_json', 'save_markdown']\n"
          ]
        }
      ],
      "source": [
        "# Try to import MRP modules, fallback to inline definitions if needed\n",
        "try:\n",
        "    from compile import TEMPLATES, main as compile_main\n",
        "    from run_job import run, validate_spec, AGENT_REGISTRY, REDUCE_REGISTRY, PRODUCE_REGISTRY\n",
        "    \n",
        "    print(\"✅ MRP modules imported successfully\")\n",
        "    print(f\"🔧 Available agents: {list(AGENT_REGISTRY.keys())}\")\n",
        "    print(f\"📊 Available reducers: {list(REDUCE_REGISTRY.keys())}\")\n",
        "    print(f\"📤 Available producers: {list(PRODUCE_REGISTRY.keys())}\")\n",
        "    \n",
        "    IMPORT_SUCCESS = True\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"⚠️  Import failed: {e}\")\n",
        "    print(\"🔧 Using fallback inline demo code...\")\n",
        "    \n",
        "    IMPORT_SUCCESS = False\n",
        "    \n",
        "    # Inline fallback code for demo purposes\n",
        "    TEMPLATES = {\n",
        "        \"run the biological data processing given the data\": '''\n",
        "job: bio_hello\n",
        "description: \"Demo: uppercase + filter length>=5, then aggregate stats\"\n",
        "map:\n",
        "  agent: process_bio\n",
        "  params:\n",
        "    min_len: 5\n",
        "  data:\n",
        "    - [\"ATCG\",\"GCTA\",\"TGCA\",\"CGAT\",\"AATG\"]\n",
        "    - [\"PROTEIN_A\",\"ENZYME_B\",\"RECEPTOR_C\",\"KINASE_D\"]\n",
        "    - [\"ATP\",\"NADH\",\"GLUCOSE\",\"GLYCOGEN\",\"LACTATE\"]\n",
        "    - [\"HEMOGLOBIN\",\"INSULIN\",\"COLLAGEN\",\"KERATIN\"]\n",
        "reduce:\n",
        "  op: stats\n",
        "produce:\n",
        "  op: print\n",
        "''',\n",
        "        \"save biological data processing to json\": '''\n",
        "job: bio_hello\n",
        "description: \"Demo: uppercase + filter length>=5, then aggregate stats\"\n",
        "map:\n",
        "  agent: process_bio\n",
        "  params:\n",
        "    min_len: 5\n",
        "  data:\n",
        "    - [\"ATCG\",\"GCTA\",\"TGCA\",\"CGAT\",\"AATG\"]\n",
        "    - [\"PROTEIN_A\",\"ENZYME_B\",\"RECEPTOR_C\",\"KINASE_D\"]\n",
        "    - [\"ATP\",\"NADH\",\"GLUCOSE\",\"GLYCOGEN\",\"LACTATE\"]\n",
        "    - [\"HEMOGLOBIN\",\"INSULIN\",\"COLLAGEN\",\"KERATIN\"]\n",
        "reduce:\n",
        "  op: stats\n",
        "produce:\n",
        "  op: save_json\n",
        "  path: bio_results.json\n",
        "'''\n",
        "    }\n",
        "    \n",
        "    # Simplified agent for demo\n",
        "    def agent_process_bio(chunk, params):\n",
        "        \"\"\"Process biological data: uppercase + filter by length\"\"\"\n",
        "        min_len = params.get('min_len', 0)\n",
        "        upper = [s.upper() for s in chunk]\n",
        "        processed = [s for s in upper if len(s) >= min_len]\n",
        "        return {\n",
        "            \"input_size\": len(chunk),\n",
        "            \"kept_size\": len(processed),\n",
        "            \"kept\": processed,\n",
        "        }\n",
        "    \n",
        "    def reduce_stats(results):\n",
        "        \"\"\"Aggregate results from all shards\"\"\"\n",
        "        total_input = sum(r[\"input_size\"] for r in results)\n",
        "        total_kept = sum(r[\"kept_size\"] for r in results)\n",
        "        kept_flat = []\n",
        "        for r in results:\n",
        "            kept_flat.extend(r[\"kept\"])\n",
        "        return {\n",
        "            \"total_input\": total_input,\n",
        "            \"total_kept\": total_kept,\n",
        "            \"kept\": kept_flat,\n",
        "            \"shards\": results,\n",
        "        }\n",
        "    \n",
        "    AGENT_REGISTRY = {\"process_bio\": agent_process_bio}\n",
        "    REDUCE_REGISTRY = {\"stats\": reduce_stats}\n",
        "    PRODUCE_REGISTRY = {\"print\": print, \"save_json\": lambda x, s: print(\"[Simulated] Saved to JSON\")}\n",
        "    \n",
        "    print(\"✅ Fallback demo setup complete\")\n",
        "    print(f\"🔧 Available agents: {list(AGENT_REGISTRY.keys())}\")\n",
        "    print(f\"📊 Available reducers: {list(REDUCE_REGISTRY.keys())}\")\n",
        "    print(f\"📤 Available producers: {list(PRODUCE_REGISTRY.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Generated YAML: demo_console.yml\n",
            "\n",
            "📄 Generated YAML Specification:\n",
            "========================================\n",
            "job: bio_hello\n",
            "description: \"Demo: uppercase + filter length>=5, then aggregate stats\"\n",
            "map:\n",
            "  agent: process_bio\n",
            "  params:\n",
            "    min_len: 5\n",
            "  data:\n",
            "    - [\"ATCG\",\"GCTA\",\"TGCA\",\"CGAT\",\"AATG\"]\n",
            "    - [\"PROTEIN_A\",\"ENZYME_B\",\"RECEPTOR_C\",\"KINASE_D\"]\n",
            "    - [\"ATP\",\"NADH\",\"GLUCOSE\",\"GLYCOGEN\",\"LACTATE\"]\n",
            "    - [\"HEMOGLOBIN\",\"INSULIN\",\"COLLAGEN\",\"KERATIN\"]\n",
            "reduce:\n",
            "  op: stats\n",
            "produce:\n",
            "  op: print\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Function to generate YAML from natural language\n",
        "def generate_yaml(query, output_file=\"temp_job.yml\"):\n",
        "    \"\"\"Generate YAML specification from natural language query.\"\"\"\n",
        "    if query not in TEMPLATES:\n",
        "        print(f\"❌ Error: No template for query: {query!r}\")\n",
        "        return None\n",
        "    \n",
        "    yaml_content = TEMPLATES[query]\n",
        "    \n",
        "    # Only write file if we have write permissions\n",
        "    try:\n",
        "        output_path = Path(output_file)  # Use current directory\n",
        "        output_path.write_text(yaml_content, encoding=\"utf-8\")\n",
        "        print(f\"✅ Generated YAML: {output_file}\")\n",
        "    except (PermissionError, FileNotFoundError) as e:\n",
        "        print(f\"⚠️  Could not write file ({e}), showing content only\")\n",
        "        output_path = None\n",
        "    \n",
        "    return yaml_content, output_path\n",
        "\n",
        "# Generate YAML for console output\n",
        "console_query = \"run the biological data processing given the data\"\n",
        "yaml_content, yaml_path = generate_yaml(console_query, \"demo_console.yml\")\n",
        "\n",
        "print(\"\\n📄 Generated YAML Specification:\")\n",
        "print(\"=\" * 40)\n",
        "print(yaml_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Simulating Parallel MapReduce Job...\n",
            "========================================\n",
            "▶ Job: bio_hello\n",
            "  - agent: process_bio\n",
            "  - shards: 4\n",
            "  - reduce: stats\n",
            "  - produce: print\n",
            "    ✓ shard 2 done\n",
            "    ✓ shard 0 done\n",
            "    ✓ shard 1 done\n",
            "    ✓ shard 3 done\n",
            "\n",
            "📊 Final Results:\n",
            "{\n",
            "  \"total_input\": 18,\n",
            "  \"total_kept\": 11,\n",
            "  \"kept\": [\n",
            "    \"PROTEIN_A\",\n",
            "    \"ENZYME_B\",\n",
            "    \"RECEPTOR_C\",\n",
            "    \"KINASE_D\",\n",
            "    \"GLUCOSE\",\n",
            "    \"GLYCOGEN\",\n",
            "    \"LACTATE\",\n",
            "    \"HEMOGLOBIN\",\n",
            "    \"INSULIN\",\n",
            "    \"COLLAGEN\",\n",
            "    \"KERATIN\"\n",
            "  ],\n",
            "  \"shards\": [\n",
            "    {\n",
            "      \"input_size\": 5,\n",
            "      \"kept_size\": 0,\n",
            "      \"kept\": []\n",
            "    },\n",
            "    {\n",
            "      \"input_size\": 4,\n",
            "      \"kept_size\": 4,\n",
            "      \"kept\": [\n",
            "        \"PROTEIN_A\",\n",
            "        \"ENZYME_B\",\n",
            "        \"RECEPTOR_C\",\n",
            "        \"KINASE_D\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"input_size\": 5,\n",
            "      \"kept_size\": 3,\n",
            "      \"kept\": [\n",
            "        \"GLUCOSE\",\n",
            "        \"GLYCOGEN\",\n",
            "        \"LACTATE\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"input_size\": 4,\n",
            "      \"kept_size\": 4,\n",
            "      \"kept\": [\n",
            "        \"HEMOGLOBIN\",\n",
            "        \"INSULIN\",\n",
            "        \"COLLAGEN\",\n",
            "        \"KERATIN\"\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import concurrent.futures\n",
        "import time\n",
        "\n",
        "# Parse the YAML and simulate execution\n",
        "spec = yaml.safe_load(yaml_content)\n",
        "\n",
        "print(\"🚀 Simulating Parallel MapReduce Job...\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"▶ Job: {spec.get('job', 'unnamed')}\")\n",
        "print(f\"  - agent: {spec['map']['agent']}\")\n",
        "print(f\"  - shards: {len(spec['map']['data'])}\")\n",
        "print(f\"  - reduce: {spec['reduce']['op']}\")\n",
        "print(f\"  - produce: {spec['produce']['op']}\")\n",
        "\n",
        "# Map phase - process shards in parallel\n",
        "agent_fn = AGENT_REGISTRY[spec['map']['agent']]\n",
        "params = spec['map'].get('params', {})\n",
        "shards = spec['map']['data']\n",
        "\n",
        "results = []\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    # Submit all shards for processing\n",
        "    future_to_shard = {executor.submit(agent_fn, shard, params): i for i, shard in enumerate(shards)}\n",
        "    \n",
        "    # Collect results as they complete\n",
        "    for future in concurrent.futures.as_completed(future_to_shard):\n",
        "        shard_idx = future_to_shard[future]\n",
        "        result = future.result()\n",
        "        results.append((shard_idx, result))\n",
        "        print(f\"    ✓ shard {shard_idx} done\")\n",
        "\n",
        "# Sort results by shard index\n",
        "results.sort(key=lambda x: x[0])\n",
        "shard_results = [r[1] for r in results]\n",
        "\n",
        "# Reduce phase\n",
        "reducer_fn = REDUCE_REGISTRY[spec['reduce']['op']]\n",
        "final_result = reducer_fn(shard_results)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n📊 Final Results:\")\n",
        "print(json.dumps(final_result, indent=2))\n",
        "\n",
        "# Store for later analysis\n",
        "result_data = final_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧬 Biological Data Processing Analysis:\n",
            "========================================\n",
            "📊 Processing Statistics:\n",
            "   • Total input items: 18\n",
            "   • Items kept (length ≥ 5): 11\n",
            "   • Filter efficiency: 61.1%\n",
            "   • Number of parallel shards: 4\n",
            "\n",
            "🧪 Filtered Biological Items:\n",
            "    1. PROTEIN_A\n",
            "    2. ENZYME_B\n",
            "    3. RECEPTOR_C\n",
            "    4. KINASE_D\n",
            "    5. GLUCOSE\n",
            "    6. GLYCOGEN\n",
            "    7. LACTATE\n",
            "    8. HEMOGLOBIN\n",
            "    9. INSULIN\n",
            "   10. COLLAGEN\n",
            "   11. KERATIN\n",
            "\n",
            "⚡ Shard Processing Details:\n",
            "   Shard 0: 5 → 0 (none)\n",
            "   Shard 1: 4 → 4 (PROTEIN_A, ENZYME_B, RECEPTOR_C, KINASE_D)\n",
            "   Shard 2: 5 → 3 (GLUCOSE, GLYCOGEN, LACTATE)\n",
            "   Shard 3: 4 → 4 (HEMOGLOBIN, INSULIN, COLLAGEN, KERATIN)\n",
            "\n",
            "📋 Structured Results:\n"
          ]
        },
        {
          "data": {
            "application/json": {
              "kept": [
                "PROTEIN_A",
                "ENZYME_B",
                "RECEPTOR_C",
                "KINASE_D",
                "GLUCOSE",
                "GLYCOGEN",
                "LACTATE",
                "HEMOGLOBIN",
                "INSULIN",
                "COLLAGEN",
                "KERATIN"
              ],
              "shards": [
                {
                  "input_size": 5,
                  "kept": [],
                  "kept_size": 0
                },
                {
                  "input_size": 4,
                  "kept": [
                    "PROTEIN_A",
                    "ENZYME_B",
                    "RECEPTOR_C",
                    "KINASE_D"
                  ],
                  "kept_size": 4
                },
                {
                  "input_size": 5,
                  "kept": [
                    "GLUCOSE",
                    "GLYCOGEN",
                    "LACTATE"
                  ],
                  "kept_size": 3
                },
                {
                  "input_size": 4,
                  "kept": [
                    "HEMOGLOBIN",
                    "INSULIN",
                    "COLLAGEN",
                    "KERATIN"
                  ],
                  "kept_size": 4
                }
              ],
              "total_input": 18,
              "total_kept": 11
            },
            "text/plain": [
              "<IPython.core.display.JSON object>"
            ]
          },
          "metadata": {
            "application/json": {
              "expanded": true,
              "root": "root"
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Analyze the biological data processing results\n",
        "print(\"🧬 Biological Data Processing Analysis:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "total_input = result_data['total_input']\n",
        "total_kept = result_data['total_kept']\n",
        "efficiency = (total_kept / total_input) * 100\n",
        "\n",
        "print(f\"📊 Processing Statistics:\")\n",
        "print(f\"   • Total input items: {total_input}\")\n",
        "print(f\"   • Items kept (length ≥ 5): {total_kept}\")\n",
        "print(f\"   • Filter efficiency: {efficiency:.1f}%\")\n",
        "print(f\"   • Number of parallel shards: {len(result_data['shards'])}\")\n",
        "\n",
        "print(f\"\\n🧪 Filtered Biological Items:\")\n",
        "for i, item in enumerate(result_data['kept'], 1):\n",
        "    print(f\"   {i:2d}. {item}\")\n",
        "\n",
        "print(f\"\\n⚡ Shard Processing Details:\")\n",
        "for i, shard in enumerate(result_data['shards']):\n",
        "    kept_items = ', '.join(shard['kept']) if shard['kept'] else 'none'\n",
        "    print(f\"   Shard {i}: {shard['input_size']} → {shard['kept_size']} ({kept_items})\")\n",
        "\n",
        "# Display as structured JSON\n",
        "print(\"\\n📋 Structured Results:\")\n",
        "display(JSON(result_data, expanded=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧬 What Makes This System 'Agentic':\n",
            "========================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "- 🤖 **Agents = Map Workers**: Each shard processed by named agent functions"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "- 🔌 **Registry Pattern**: Pluggable agents, reducers, producers without changing core runtime"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "- 📋 **Spec-Driven**: All behavior declared in YAML, validated before execution"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "- ⚡ **Parallel Execution**: Automatic work distribution across available cores"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "- 🧩 **Composable**: Easy to add new processing patterns and output formats"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "- 🔄 **Event-Driven**: Clear map → reduce → produce pipeline"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "- 📊 **Multiple Outputs**: Console, JSON, Markdown reports from same specification"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🤖 Agent Registry System:\n",
            "   • Available Map Agents: ['process_bio']\n",
            "   • Available Reducers: ['stats']\n",
            "   • Available Producers: ['print', 'save_json', 'save_markdown']\n",
            "\n",
            "📊 Demo Processing Results:\n",
            "   • Total input items: 18\n",
            "   • Items kept (length ≥ 5): 11\n",
            "   • Filter efficiency: 61.1%\n",
            "   • Parallel shards: 4\n"
          ]
        }
      ],
      "source": [
        "print(\"🧬 What Makes This System 'Agentic':\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "agentic_features = [\n",
        "    \"🤖 **Agents = Map Workers**: Each shard processed by named agent functions\",\n",
        "    \"🔌 **Registry Pattern**: Pluggable agents, reducers, producers without changing core runtime\",\n",
        "    \"📋 **Spec-Driven**: All behavior declared in YAML, validated before execution\", \n",
        "    \"⚡ **Parallel Execution**: Automatic work distribution across available cores\",\n",
        "    \"🧩 **Composable**: Easy to add new processing patterns and output formats\",\n",
        "    \"🔄 **Event-Driven**: Clear map → reduce → produce pipeline\",\n",
        "    \"📊 **Multiple Outputs**: Console, JSON, Markdown reports from same specification\"\n",
        "]\n",
        "\n",
        "for feature in agentic_features:\n",
        "    display(Markdown(f\"- {feature}\"))\n",
        "\n",
        "# Show registry extensibility\n",
        "print(\"\\n🤖 Agent Registry System:\")\n",
        "print(f\"   • Available Map Agents: {list(AGENT_REGISTRY.keys())}\")\n",
        "print(f\"   • Available Reducers: {list(REDUCE_REGISTRY.keys())}\")\n",
        "print(f\"   • Available Producers: {list(PRODUCE_REGISTRY.keys())}\")\n",
        "\n",
        "# Show processing summary\n",
        "print(f\"\\n📊 Demo Processing Results:\")\n",
        "print(f\"   • Total input items: {total_input}\")\n",
        "print(f\"   • Items kept (length ≥ 5): {total_kept}\")\n",
        "print(f\"   • Filter efficiency: {efficiency:.1f}%\")\n",
        "print(f\"   • Parallel shards: {len(result_data['shards'])}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
